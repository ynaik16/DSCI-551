# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17JkFyzVAHkLweHnDNgqe1ae9aIUiUm6f
"""

!apt-get install openjdk-8-jdk

!wget -q https://apachemirror.wuchna.com/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz

!tar xf spark-3.1.1-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("HW5") \
    .getOrCreate()

from pyspark import SparkContext

city_file = "/content/drive/MyDrive/Colab Notebooks/DS551/city.json"
country_file = "/content/drive/MyDrive/Colab Notebooks/DS551/country.json"
language_file = "/content/drive/MyDrive/Colab Notebooks/DS551/countrylanguage.json"

sc = spark.sparkContext

country_df = spark.read.json(country_file)

country_df.show()

"""##### query-a"""

answer = country_df.select(country_df['Name']).where(country_df['Continent']=="North America").show()



"""####### query -b"""

city_df = spark.read.json(city_file)

query_b_df = country_df.join(city_df, country_df.Capital == city_df.ID).select(country_df.Name,city_df.Name).show()





"""########### query c"""

answer_c = country_df.select(country_df['Continent']).distinct().show()



"""######### query d"""

cl_df = spark.read.json(language_file)

cl_df.show()

answerd_d = cl_df.select(cl_df['Language']).filter(cl_df['CountryCode']=='CAN').show()

"""######### query 3"""

import pyspark.sql.functions as fc

country_df.select(country_df['Continent']).show()

answer_e = country_df.select(country_df['Continent'], country_df['LifeExpectancy']) \
            .groupBy(country_df['Continent']) \
            .agg(fc.avg(country_df['LifeExpectancy']).alias('avg_le'),fc.count('*').alias('count'))

final_ans = answer_e[answer_e['count'] >= 20].orderBy(fc.desc('count')).limit(1)

final_ans = final_ans.select(final_ans['Continent'],final_ans['avg_le']).show()







"""# ###### Rdd_a"""

sc = spark.sparkContext

rdd_a = country_df.rdd.map(list)

print(rdd_a.map(lambda x: (x[11],x[3])).filter(lambda x: x[1] == "North America").map(lambda x: x[0]).collect())







"""####### rdd_b"""

rdd_country = country_df.rdd.map(list)
rdd_city = city_df.rdd.map(list)

#rdd_country.take(1)

a = rdd_country.map(lambda x: (x[11],x[0]))
#a.collect()

#type(a)

#rdd_city.first()

b = rdd_city.map(lambda x: (x[3],x[2]))
#b.collect()

#type(b)

new_rdd = a.join(b)
rdd_query_b_answer = new_rdd.map(lambda x: (x[0],x[1][0])).collect()

print(rdd_query_b_answer)

